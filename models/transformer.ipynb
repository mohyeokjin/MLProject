{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "center_size = 300\n",
    "batch_size = 32\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_folder, label_folder, transform):\n",
    "        self.image_folder = image_folder\n",
    "        self.label_folder = label_folder\n",
    "        self.transform = transform\n",
    "\n",
    "        self.image_files = sorted(os.listdir(image_folder))\n",
    "        self.label_files = sorted(os.listdir(label_folder))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.image_folder, self.image_files[idx])\n",
    "        label_name = os.path.join(self.label_folder, self.label_files[idx])\n",
    "\n",
    "        img = Image.open(img_name)\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        with open(label_name, 'r') as label_file:\n",
    "            label = label_file.read()\n",
    "            label = label.split()\n",
    "            x = float(label[0])\n",
    "            y = float(label[1])\n",
    "\n",
    "        return img, torch.tensor([x, y], dtype=torch.float32)\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, img_size:int, patch_size:int, in_chans:int=3, emb_dim:int=48):\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        self.proj = nn.Conv2d(\n",
    "            in_chans,\n",
    "            emb_dim,\n",
    "            kernel_size=patch_size,\n",
    "            stride=patch_size\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            x = self.proj(x)\n",
    "            x = x.flatten(2)\n",
    "            x = x.transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, dim:int, n_heads:int=8, qkv_bias:bool=True, attn_p:float=0.01, proj_p:float=0.01):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.dim = dim\n",
    "        self.head_dim = dim // n_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim*3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_p)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, n_tokens, x_dim = x.shape\n",
    "\n",
    "        if x_dim != self.dim:\n",
    "            raise ValueError\n",
    "        if self.dim != self.head_dim*self.n_heads:\n",
    "            raise ValueError\n",
    "\n",
    "        qkv = self.qkv(x)\n",
    "        qkv = qkv.reshape(batch_size, n_tokens, 3, self.n_heads, self.head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 3)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        k_t = k.transpose(-2, -1)\n",
    "        dot_product = (q @ k_t) * self.scale\n",
    "\n",
    "        dot_product = dot_product[:, :, :, :center_size]\n",
    "        print(f\"dot_product shape : {dot_product.shape}\")\n",
    "        attn = dot_product.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "        weighted_avg = attn @ v\n",
    "        weighted_avg = weighted_avg.transpose(1, 2)\n",
    "\n",
    "        weighted_avg = weighted_avg.flatten(2)\n",
    "        x = self.proj(weighted_avg)\n",
    "        x = self.proj_drop(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class MultiHeadSelfAttentionLayer(nn.Module):\n",
    "    def __init__(self, dim:int, n_heads:int=8, qkv_bias:bool=True, attn_p:float=0.01, proj_p:float=0.01):\n",
    "        super(MultiHeadSelfAttentionLayer, self).__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.dim = dim\n",
    "        self.head_dim = dim // n_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim*3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_p)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_p)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, n_tokens, x_dim = x.shape\n",
    "\n",
    "        if x_dim != self.dim:\n",
    "            raise ValueError\n",
    "        if self.dim != self.head_dim * self.n_heads:\n",
    "            raise ValueError\n",
    "\n",
    "        qkv = self.qkv(x)\n",
    "        qkv = qkv.reshape(batch_size, n_tokens, 3, self.n_heads, self.head_dim)\n",
    "        print(f\"batch_size : {batch_size}\")\n",
    "        print(f\"n_tokens : {n_tokens}\")\n",
    "        print(f\"n_heads : {self.n_heads}\")\n",
    "        print(f\"head_dim : {self.head_dim}\")\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
    "        print(f\"qkv shape : {qkv.shape}\")\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        print(f\"q shape: {q.shape}\")\n",
    "        print(f\"k shape: {k.shape}\")\n",
    "        print(f\"v shape: {v.shape}\")\n",
    "\n",
    "        k_t = k.transpose(-2, -1)\n",
    "        dot_product = (q @ k_t) * self.scale\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(0).unsqueeze(0).expand(batch_size, self.n_heads, -1, -1)\n",
    "            print(f\"after mask : {mask.shape}\")\n",
    "\n",
    "            dot_product = dot_product[:, :, :18, :18]  \n",
    "            attn = dot_product.softmax(dim=-1)\n",
    "            attn = attn.masked_fill(mask, float('-inf'))\n",
    "        else:\n",
    "            attn = dot_product.softmax(dim=-1)\n",
    "            print(f\"dot_product shape : {dot_product.shape}\")\n",
    "\n",
    "        attn = self.attn_drop(attn)\n",
    "        weighted_avg = attn @ v\n",
    "        weighted_avg = weighted_avg.transpose(1, 2)\n",
    "\n",
    "        weighted_avg = weighted_avg.flatten(2)\n",
    "        x = self.proj(weighted_avg)\n",
    "        x = self.proj_drop(x)\n",
    "\n",
    "        return x, attn\n",
    "\n",
    "class PositionwiseFeedforwardLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim, pf_dim, dropout_ratio):\n",
    "        super(PositionwiseFeedforwardLayer, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(hidden_dim, pf_dim)\n",
    "        self.fc2 = nn.Linear(pf_dim, hidden_dim)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(self.gelu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim, n_heads, pf_dim, dropout_ratio):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hidden_dim)\n",
    "        self.ff_layer_norm = nn.LayerNorm(hidden_dim)\n",
    "        self.self_attention = MultiHeadSelfAttentionLayer(hidden_dim, n_heads, dropout_ratio)\n",
    "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hidden_dim, pf_dim, dropout_ratio)\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        _src, _ = self.self_attention(src, mask=src_mask)  \n",
    "        src = self.self_attn_layer_norm(src + self.dropout(_src))\n",
    "        _src = self.positionwise_feedforward(src)\n",
    "        src = self.ff_layer_norm(src + self.dropout(_src))\n",
    "        return src\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim, n_heads, pf_dim, dropout_ratio):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hidden_dim)\n",
    "        self.enc_attn_layer_norm = nn.LayerNorm(hidden_dim)\n",
    "        self.ff_layer_norm = nn.LayerNorm(hidden_dim)\n",
    "        self.self_attention = MultiHeadSelfAttentionLayer(hidden_dim, n_heads, dropout_ratio)\n",
    "        self.encoder_attention = MultiHeadSelfAttentionLayer(hidden_dim, n_heads, dropout_ratio)\n",
    "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hidden_dim, pf_dim, dropout_ratio)\n",
    "        self.dropout = nn.Dropout(dropout_ratio)\n",
    "\n",
    "    def forward(self, trg, enc_src, trg_mask, mask):  \n",
    "        _trg, _ = self.self_attention(trg, mask=trg_mask) \n",
    "        trg = self.self_attn_layer_norm(trg + self.dropout(_trg))\n",
    "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, mask=mask)  \n",
    "        trg = self.enc_attn_layer_norm(trg + self.dropout(_trg))\n",
    "        _trg = self.positionwise_feedforward(trg)\n",
    "        trg = self.ff_layer_norm(trg + self.dropout(_trg))\n",
    "        return trg, attention\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, input_dim:int, mlp_hidden_dim:int, num_head:int=8, dropout:float=0.):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.norm1 = nn.LayerNorm(input_dim)\n",
    "        self.msa = MultiHeadSelfAttentionLayer(input_dim, n_heads=num_head)\n",
    "        self.norm2 = nn.LayerNorm(input_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim, mlp_hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(mlp_hidden_dim, input_dim),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "        self.encoder_layer = EncoderLayer(input_dim, num_head, mlp_hidden_dim, dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.msa(self.norm1(x))\n",
    "        out = self.mlp(self.norm2(out)) + out\n",
    "        out = self.encoder_layer(out, src_mask=None)\n",
    "        return out\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, input_dim:int, mlp_hidden_dim:int, num_head:int=8, dropout:float=0.):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.norm1 = nn.LayerNorm(input_dim)\n",
    "        self.norm2 = nn.LayerNorm(input_dim)\n",
    "        self.norm3 = nn.LayerNorm(input_dim)\n",
    "\n",
    "        self.self_attention = MultiHeadSelfAttentionLayer(input_dim, n_heads=num_head)\n",
    "        self.encoder_attention = MultiHeadSelfAttentionLayer(input_dim, n_heads=num_head)\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim, mlp_hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(mlp_hidden_dim, input_dim),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "        self.decoder_layer = DecoderLayer(input_dim, num_head, mlp_hidden_dim, dropout)\n",
    "\n",
    "    def forward(self, x, enc_src, trg_mask, mask):\n",
    "        _trg, attn_dec = self.self_attention(x, mask=trg_mask)\n",
    "        trg = self.norm1(x + _trg)\n",
    "\n",
    "        _trg, attn_enc = self.encoder_attention(trg, enc_src, enc_src, mask=mask)\n",
    "        trg = self.norm2(trg + _trg)\n",
    "\n",
    "        _trg = self.mlp(trg)\n",
    "        trg = self.norm3(trg + _trg)\n",
    "\n",
    "        return trg, attn_dec, attn_enc\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, img_size, patch_size, emb_dim, mlp_hidden_dim, num_heads, dropout):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.patch_embedding = PatchEmbedding(img_size, patch_size, 3, emb_dim)\n",
    "        self.transformer_decoder = TransformerDecoder(emb_dim, mlp_hidden_dim, num_heads, dropout)\n",
    "        self.n_heads = num_heads  \n",
    "\n",
    "    def forward(self, x):\n",
    "        enc_out = self.patch_embedding(x)\n",
    "\n",
    "        max_len = center_size // 16\n",
    "        trg_mask = torch.ones(max_len, max_len).to(x.device).triu(1).bool()\n",
    "        mask = trg_mask.unsqueeze(0).expand(x.shape[0], -1, -1).contiguous()\n",
    "\n",
    "        out = self.transformer_decoder(enc_out, enc_out, trg_mask, mask)\n",
    "        return out\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((center_size, center_size)),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]) \n",
    "])\n",
    "\n",
    "image_path = \"/content/drive/MyDrive/images\"\n",
    "label_path = \"/content/drive/MyDrive/labels\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "train_dataset = CustomDataset(image_folder=image_path, label_folder=label_path, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model = TransformerModel(img_size=center_size, patch_size=16, emb_dim=48, mlp_hidden_dim=100, num_heads=8, dropout=0.1)\n",
    "model.to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 학습 루프\n",
    "for epoch in range(100):\n",
    "    total_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "\n",
    "        predicted_x, predicted_y = outputs[:, 0], outputs[:, 1]\n",
    "        true_x, true_y = labels[:, 0], labels[:, 1]\n",
    "\n",
    "        loss_x = criterion(predicted_x, true_x)\n",
    "        loss_y = criterion(predicted_y, true_y)\n",
    "        loss = loss_x + loss_y\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    average_loss = total_loss / len(train_loader)\n",
    "    print(f'Epoch [{epoch+1}/{100}], Loss: {average_loss}')\n",
    "    print(f'Predicted X: {predicted_x[0].item()}, Predicted Y: {predicted_y[0].item()}')\n",
    "    print(f'True X: {true_x[0].item()}, True Y: {true_y[0].item()}')\n",
    "\n",
    "torch.save(model.state_dict(), \"./drive/MyDrive/model_transformer_ver2.pth\")\n",
    "print(\"학습 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "테스트(영상)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "center_size = 300\n",
    "batch_size = 32\n",
    "\n",
    "model = TransformerModel(img_size=center_size, patch_size=16, emb_dim=48, mlp_hidden_dim=100, num_heads=8, dropout=0.1)\n",
    "model.load_state_dict(torch.load(\"model_transformer.pth\", map_location = torch.device('cpu')))\n",
    "model.eval()\n",
    "\n",
    "readvideo = cv2.VideoCapture(\"center_screen_recording_alone.avi\")\n",
    "cv2.namedWindow('win')\n",
    "\n",
    "while True:\n",
    "    ret, frame = readvideo.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((center_size, center_size)),\n",
    "    ])\n",
    "\n",
    "    pil_image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    input_image = transform(pil_image).unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        input_image = input_image.to(device)\n",
    "        outputs = model(input_image)\n",
    "        print(outputs)\n",
    "        outputs = outputs.squeeze().tolist()\n",
    "\n",
    "    cv2.circle(frame, (round(outputs[0] * 300), round(outputs[1] * 300)), 5, (0, 0, 255), 2)\n",
    "    cv2.imshow('win', frame)\n",
    "\n",
    "    key = cv2.waitKey(1)\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "# 리소스 해제\n",
    "readvideo.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
